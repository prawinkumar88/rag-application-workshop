"""
Streamlit UI for Vector RAG Module
Demonstrates semantic search and vector-based retrieval
"""

import streamlit as st
from datetime import datetime
from vector_rag_engine import VectorRAGEngine
from rag_engine import RAGEngine

st.set_page_config(
    page_title="Vector RAG Demo",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” Vector-based RAG Workshop")
st.markdown("*Learn semantic search with vector embeddings*")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    providers = RAGEngine.get_available_providers()
    provider = st.selectbox("Select Provider:", providers)
    
    top_k = st.slider("Documents to Retrieve:", 1, 5, 3)
    show_sources = st.checkbox("Show Sources", value=True)
    
    st.divider()
    
    st.header("ğŸ“– How It Works")
    st.markdown("""
    **Vector RAG Steps:**
    1. ğŸ“ Convert query to embedding
    2. ğŸ” Find similar documents
    3. ğŸ“Š Retrieve top-k results
    4. ğŸ¤– Generate answer
    """)

# Initialize
if "vector_rag" not in st.session_state:
    with st.spinner("Initializing vector store..."):
        ollama_model = None
        if provider == "ollama":
            from ollama_provider import OllamaProvider
            models = OllamaProvider.get_available_models()
            ollama_model = models[0] if models else "mistral"
        
        st.session_state.vector_rag = VectorRAGEngine(
            provider_type=provider,
            ollama_model=ollama_model,
            top_k=top_k
        )

rag = st.session_state.vector_rag

# Main content
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ’¬ Ask a Question")
    query = st.text_input("Enter your question:", key="query_input")
    
    if st.button("ğŸ” Search & Answer", use_container_width=True):
        if query:
            # Show search results
            st.subheader("ğŸ“Š Retrieved Documents")
            results = rag.get_search_results(query)
            
            for i, result in enumerate(results, 1):
                with st.expander(f"ğŸ“„ Document {i}: {result['metadata']['name']}"):
                    relevance = (1 - result['distance']) * 100
                    st.metric("Relevance Score", f"{relevance:.1f}%")
                    st.code(result['document'], language=None)
            
            # Get answer
            st.divider()
            st.subheader("ğŸ’¬ Generated Answer")
            with st.spinner("Generating answer..."):
                response = rag.query(query, return_sources=show_sources)
            st.success(response)

with col2:
    st.subheader("ğŸ“ Example Queries")
    examples = rag.get_example_prompts()
    
    for example in examples[:5]:
        if st.button(example, key=f"ex_{example[:20]}", use_container_width=True):
            st.session_state.query_input = example
            st.rerun()

# Comparison section
st.divider()
st.header("âš–ï¸ Vector RAG vs Prompt-based RAG")

col1, col2 = st.columns(2)

with col1:
    st.subheader("ğŸ” Vector RAG")
    st.markdown("""
    **Advantages:**
    - âœ… Scalable to large databases
    - âœ… Semantic understanding
    - âœ… Efficient retrieval
    - âœ… Reduced context size
    
    **Use Cases:**
    - Large document collections
    - Semantic search needed
    - Context window limits
    """)

with col2:
    st.subheader("ğŸ“ Prompt-based RAG")
    st.markdown("""
    **Advantages:**
    - âœ… Simple implementation
    - âœ… Complete context available
    - âœ… No indexing needed
    - âœ… Exact matches guaranteed
    
    **Use Cases:**
    - Small databases
    - Exact information needed
    - Simple queries
    """)
